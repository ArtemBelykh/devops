Вопросы и Ответы по Kubernetes (Q&A)
====================================

Этот файл содержит систематизированные вопросы и ответы из нашей переписки по настройке и использованию Kubernetes.

> **Вопрос:** А в чем прикол тогда виртуалок, если кубер будет смотреть на один и тот же сервер, просто на другие ноды?

**Ответ:** "Прикол" в **изоляции и симуляции реальной среды**. Виртуализация позволяет превратить ОДИН физический сервер в ПОЛНОЦЕННЫЙ, отказоустойчивый кластер из НЕСКОЛЬКИХ изолированных серверов (Узлов/Nodes).

1.  **Отказоустойчивость:** Если одна ВМ (Узел) "упадет" из-за сбоя в ее ОС, остальные продолжат работать. Kubernetes заметит это и автоматически перезапустит приложения (Поды) со сломанного узла на здоровом.

2.  **Гибкое управление ресурсами:** Вы можете "нарезать" ресурсы мощного физического сервера на несколько ВМ с разными характеристиками (маленький мастер, большие воркеры).

3.  **Безопасность:** Взлом одной ВМ не дает доступа к другим.

> **Вопрос:** 1\. По `containerd`: можно ли без установки репозитория Docker? 2. По Kubernetes: Почему версия 1.30, а не 1.31? 3. Что такое шаблон в Proxmox и зачем клонировать узлы? 4. Как работают 3 мастер-ноды?

**Ответ:**

1.  **Containerd и Docker:** Мы НЕ устанавливаем Docker. Мы лишь используем его репозиторий, чтобы скачать оттуда свежую и стабильную версию `containerd`. Это проверенная практика сообщества.

2.  **Версия Kubernetes:** В руководствах часто используют предыдущую стабильную версию (1.30), так как она уже "обкатана" и лишена "детских болезней" самой новой (1.31). Вы можете легко использовать 1.31, просто изменив URL в файле репозитория.

3.  **Шаблон и Клонирование:** Шаблон --- это "золотой стандарт" или мастер-копия вашей ВМ. Клонирование из шаблона экономит массу времени, гарантирует идентичность всех узлов и позволяет быстро масштабировать кластер.

4.  **Три мастер-ноды (HA):** Они не просто клоны. Они образуют распределенную систему управления. Их "мозг" (база `etcd`) синхронизирован между ними. Специальный "плавающий" IP-адрес (VIP) обеспечивает единую точку входа. Если один мастер падает, двое других продолжают работу, поддерживая кворум.

> **Вопрос:** По балансировщику (HAProxy), запустится ли какая-либо панель администрирования, чтобы визуально наблюдать?

**Ответ:** Да, у HAProxy есть встроенная страница статистики, которую нужно включить в конфиге `/etc/haprox/haprox.cfg`. Она показывает статус backend-серверов (живы/упали), количество соединений и другую полезную информацию.

```
listen stats
    bind *:8404
    mode http
    stats enable
    stats uri /stats
    stats auth admin:MySuperSecretPassword123

```

Доступ будет по адресу `http://<IP-балансировщика>:8404/stats`.

> **Вопрос:** Можно ли собирать образы для CI/CD без Docker, я его не люблю?

**Ответ:** Да, абсолютно. Термин "Docker-образ" стал нарицательным, но на самом деле это стандартный **OCI-образ**. Вы можете использовать современные инструменты, не требующие фонового демона, такие как **Podman** или **Buildah**. `Podman` --- это практически полная замена `docker` CLI, и команды будут идентичны (`podman build`, `podman push`). Это более безопасный и легковесный подход.

> **Вопрос:** По RabbitMQ: я имел в виду, чтобы на мастер-ноде создалась отдельная воркер-нода для RabbitMQ.

**Ответ:** Это критически важное заблуждение. **Категорически нельзя запускать рабочие приложения (как RabbitMQ) на мастер-нодах.**

-   **Мастер-ноды** --- это "мозг" кластера, они отвечают только за управление. Запуск на них приложений может привести к отказу всего кластера.

-   **Воркер-ноды** --- это "мышцы", они предназначены для запуска приложений. Вы не "создаете воркер-ноду на мастере". Мастер и Воркер --- это роли **отдельных, независимых виртуальных машин**. Kubernetes сам разместит под с RabbitMQ на одной из существующих Воркер-нод, где есть свободные ресурсы.

> **Вопрос:** Зачем Kafka нужен ZooKeeper?

**Ответ:** В классической архитектуре Kafka, ZooKeeper --- это внешний **координационный сервис** или "прораб" для брокеров Kafka. Он отвечает за:

1.  **Выбор главного брокера-контроллера.**

2.  **Хранение списка живых брокеров.**

3.  **Хранение конфигурации топиков.**

4.  **Хранение списков доступа (ACL).** В современных версиях Kafka появился режим **KRaft**, который позволяет брокерам самим выполнять эти задачи, избавляясь от необходимости в ZooKeeper.

> **Вопрос:** Как ввести в эксплуатацию еще один физический сервер и поместить туда кубер с проксмоксом?

**Ответ:** Вы не создаете второй кластер, а **расширяете существующий**.

1.  Убедитесь, что оба сервера находятся в одной локальной сети.

2.  На втором сервере вы также устанавливаете Proxmox.

3.  Вы создаете новые ВМ на втором сервере (из того же шаблона) и присоединяете их к **существующему кластеру** командой `kubeadm join`, указывая на IP-адрес мастера или VIP вашего первого кластера.

4.  Для максимальной надежности вы можете распределить мастер-ноды по разным физическим серверам.

> **Вопрос:** Что такое пространство имен (namespace) в кубере?

**Ответ:** Namespace --- это механизм для создания **логических "виртуальных кластеров"** внутри вашего одного физического кластера. Аналогия --- папки на диске. Они используются для:

1.  **Организации:** Группировки ресурсов по проектам.

2.  **Изоляции:** Разделения сред `prod`, `staging`, `dev`.

3.  **Безопасности:** Ограничения доступа для разных команд.

4.  **Квот:** Ограничения потребления ресурсов (CPU/RAM).

> **Вопрос:** Как создать ещё один, отдельный кластер?

**Ответ:** Вы повторяете весь процесс создания первого кластера, но с **новым, полностью изолированным набором виртуальных машин**.

1.  Спланируйте новый диапазон IP-адресов и новые hostname.

2.  Склонируйте новый набор ВМ из шаблона.

3.  Настройте на них новые IP и hostname.

4.  Запустите `kubeadm init` на новом мастере.

5.  Чтобы управлять обоими кластерами с одной машины, вам нужно объединить их конфигурационные файлы (`~/.kube/config`) и использовать **контексты** (`kubectl config use-context ...`) для переключения между ними.
